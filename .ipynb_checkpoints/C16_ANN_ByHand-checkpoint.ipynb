{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# 2. ANN model\n",
    "###############\n",
    "\n",
    "class Neural_Network(object):\n",
    "    def __init__(self):\n",
    "        # Define Hyperparameters\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 128\n",
    "\n",
    "        # Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize, self.hiddenLayerSize)\n",
    "        self.W3 = np.random.randn(self.hiddenLayerSize, self.outputLayerSize)\n",
    "\n",
    "        self.b1 = np.random.randn(1, self.hiddenLayerSize)\n",
    "        self.b2 = np.random.randn(1, self.hiddenLayerSize)\n",
    "        self.b3 = np.random.randn(1, self.outputLayerSize)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1) + self.b1\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2) + self.b2\n",
    "        self.a3 = self.sigmoid(self.z3)\n",
    "        self.z4 = np.dot(self.a3, self.W3) + self.b3\n",
    "        yHat = self.z4\n",
    "        return yHat\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        # Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoidPrime(self, z):\n",
    "        # Gradient of sigmoid\n",
    "        return np.exp(-z) / ((1 + np.exp(-z)) ** 2)\n",
    "\n",
    "    def costFunction(self, X, y):\n",
    "        # Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5 * sum((y - self.yHat) ** 2)\n",
    "        return J[0]\n",
    "\n",
    "    def costFunctionPrime(self, X, y):\n",
    "        # Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "\n",
    "        # don't get sigmoid prime in output layer.\n",
    "        # we don't use activation function in output layer\n",
    "        delta4 = -(y - self.yHat)\n",
    "        dJdW3 = np.dot(self.a3.T, delta4)\n",
    "        dJdb3 = np.mean(delta4, axis=0)\n",
    "\n",
    "        # after then, we need sigmoid prime because of activate function(sigmoid)\n",
    "        delta3 = np.dot(delta4, self.W3.T) * self.sigmoidPrime(self.z3)\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        dJdb2 = np.mean(delta3, axis=0)\n",
    "\n",
    "        delta2 = np.dot(delta3, self.W2.T) * self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)\n",
    "        dJdb1 = np.mean(delta2, axis=0)\n",
    "\n",
    "        return dJdW1, dJdW2, dJdW3, dJdb1, dJdb2, dJdb3\n",
    "\n",
    "    def gradient_descent(self, lr, dJdW1, dJdW2, dJdW3, dJdb1, dJdb2, dJdb3):\n",
    "\n",
    "        self.W1 = self.W1 - lr * dJdW1\n",
    "        self.W2 = self.W2 - lr * dJdW2\n",
    "        self.W3 = self.W3 - lr * dJdW3\n",
    "\n",
    "        self.b1 = self.b1 - lr * dJdb1\n",
    "        self.b2 = self.b2 - lr * dJdb2\n",
    "        self.b3 = self.b3 - lr * dJdb3\n",
    "\n",
    "    # not necessary but worthy to reconsider what is hyper parameter\n",
    "    def opt_hyper_params(self, X, y):\n",
    "        best_cost = 100000\n",
    "        best_params = {'input_dim': None, 'hidden_dim': None}\n",
    "\n",
    "        for dim_in in range(1, 30):\n",
    "            for dim_hid in range(128, 527, 40):\n",
    "                self.inputLayerSize = dim_in\n",
    "                self.hiddenLayerSize = dim_hid\n",
    "\n",
    "                cost = self.costFunction(X, y)\n",
    "\n",
    "                if cost < best_cost:\n",
    "                    best_params['input_dim'] = dim_in\n",
    "                    best_params['hidden_dim'] = dim_hid\n",
    "                    best_cost = cost\n",
    "\n",
    "        self.inputLayerSize = best_params['input_dim']\n",
    "        self.hiddenLayerSize = best_params['hidden_dim']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# 3. train model\n",
    "#################\n",
    "def train(epoch, lr, X, y):\n",
    "    NN = Neural_Network()\n",
    "    print('Neaural Network is formed.')\n",
    "\n",
    "    # not necessary\n",
    "    NN.opt_hyper_params(X, y)\n",
    "    print('\\nHyper parameter optimization is done.')\n",
    "    print(' Input layer size: {} \\t Hidden layer size: {}'.format(NN.inputLayerSize, NN.hiddenLayerSize))\n",
    "\n",
    "    print('\\nTraining ANN...')\n",
    "\n",
    "\n",
    "    costs = []  # list of cost for each epoch\n",
    "    # start training\n",
    "    for epoch in range(epoch + 1):\n",
    "        # calculate gradients\n",
    "        dJdW1, dJdW2, dJdW3, dJdb1, dJdb2, dJdb3 = NN.costFunctionPrime(X, y)\n",
    "\n",
    "        # update Weight with gradient\n",
    "        NN.gradient_descent(lr, dJdW1, dJdW2, dJdW3, dJdb1, dJdb2, dJdb3)\n",
    "\n",
    "        cost = NN.costFunction(X, y)\n",
    "        costs.append(cost)\n",
    "        print(\"epoch: {}, cost: {}\".format(epoch, cost))\n",
    "\n",
    "    # plot train process, showing cost of each epoch\n",
    "    x_axis = np.arange(0, epoch + 1)\n",
    "    plt.plot(x_axis, costs)\n",
    "    plt.xlabel = 'epoch'\n",
    "    plt.ylabel = 'cost'\n",
    "    plt.grid(1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# 4. generate data\n",
    "###################\n",
    "# X = (hours sleeping, hours studying), y = Score on test\n",
    "X = np.array(([3,5], [5,1], [10,2]), dtype=float)\n",
    "y = np.array(([75], [82], [93]), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neaural Network is formed.\n",
      "\n",
      "Hyper parameter optimization is done.\n",
      " Input layer size: 1 \t Hidden layer size: 128\n",
      "\n",
      "Training ANN...\n",
      "epoch: 0, cost: 4918.09576909\n",
      "epoch: 1, cost: 3054.7444196\n",
      "epoch: 2, cost: 1934.5904311\n",
      "epoch: 3, cost: 1217.66619783\n",
      "epoch: 4, cost: 764.840396623\n",
      "epoch: 5, cost: 492.597740348\n",
      "epoch: 6, cost: 323.332382584\n",
      "epoch: 7, cost: 220.946724168\n",
      "epoch: 8, cost: 160.504907037\n",
      "epoch: 9, cost: 123.658985146\n",
      "epoch: 10, cost: 100.606663733\n",
      "epoch: 11, cost: 85.8498317572\n",
      "epoch: 12, cost: 76.1454213836\n",
      "epoch: 13, cost: 69.5442608754\n",
      "epoch: 14, cost: 64.8665359435\n",
      "epoch: 15, cost: 61.3957394231\n",
      "epoch: 16, cost: 58.6953055642\n",
      "epoch: 17, cost: 56.4975468884\n",
      "epoch: 18, cost: 54.636180263\n",
      "epoch: 19, cost: 53.0055324736\n",
      "epoch: 20, cost: 51.5362084141\n",
      "epoch: 21, cost: 50.1808427454\n",
      "epoch: 22, cost: 48.905872958\n",
      "epoch: 23, cost: 47.6867863447\n",
      "epoch: 24, cost: 46.5053064474\n",
      "epoch: 25, cost: 45.3476253812\n",
      "epoch: 26, cost: 44.2031621171\n",
      "epoch: 27, cost: 43.0635431044\n",
      "epoch: 28, cost: 41.921656158\n",
      "epoch: 29, cost: 40.7707827247\n",
      "epoch: 30, cost: 39.6039898886\n",
      "epoch: 31, cost: 38.4141615297\n",
      "epoch: 32, cost: 37.1952528179\n",
      "epoch: 33, cost: 35.9454394653\n",
      "epoch: 34, cost: 34.6723110934\n",
      "epoch: 35, cost: 33.3982324438\n",
      "epoch: 36, cost: 32.1607222627\n",
      "epoch: 37, cost: 31.0027194664\n",
      "epoch: 38, cost: 29.9562600781\n",
      "epoch: 39, cost: 29.0322199439\n",
      "epoch: 40, cost: 28.2229387448\n",
      "epoch: 41, cost: 27.5115083962\n",
      "epoch: 42, cost: 26.879434987\n",
      "epoch: 43, cost: 26.3102966177\n",
      "epoch: 44, cost: 25.7907024874\n",
      "epoch: 45, cost: 25.3101007801\n",
      "epoch: 46, cost: 24.8602655229\n",
      "epoch: 47, cost: 24.4347794935\n",
      "epoch: 48, cost: 24.0286023373\n",
      "epoch: 49, cost: 23.637733754\n",
      "epoch: 50, cost: 23.2589598107\n"
     ]
    }
   ],
   "source": [
    "train(50, 0.001, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
